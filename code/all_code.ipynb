{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_proj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDehtgTxC7Ff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "147c821f-c2c9-4c3b-d39c-325e326212dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!unzip \"/content/drive/My Drive/data.zip\"\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/My Drive/data.zip\n",
            "   creating: data/davis/\n",
            "  inflating: data/davis/drug-drug_similarities_2D.txt  \n",
            "  inflating: data/davis/drug-target_interaction_affinities_Kd__Davis_et_al.2011v1.txt  \n",
            "   creating: data/davis/folds/\n",
            "  inflating: data/davis/folds/test_fold_setting1.txt  \n",
            "  inflating: data/davis/folds/train_fold_setting1.txt  \n",
            "  inflating: data/davis/ligands_can.txt  \n",
            "  inflating: data/davis/ligands_iso.txt  \n",
            "  inflating: data/davis/proteins.txt  \n",
            "  inflating: data/davis/target-target_similarities_WS.txt  \n",
            "  inflating: data/davis/Y            \n",
            "   creating: data/kiba/\n",
            "   creating: data/kiba/folds/\n",
            "  inflating: data/kiba/folds/test_fold_setting1.txt  \n",
            "  inflating: data/kiba/folds/train_fold_setting1.txt  \n",
            "  inflating: data/kiba/kiba_binding_affinity_v2.txt  \n",
            "  inflating: data/kiba/kiba_drug_sim.txt  \n",
            "  inflating: data/kiba/kiba_target_sim.txt  \n",
            "  inflating: data/kiba/ligands_can.txt  \n",
            "  inflating: data/kiba/ligands_iso.txt  \n",
            "  inflating: data/kiba/proteins.txt  \n",
            "  inflating: data/kiba/Y             \n",
            "  inflating: data/README.md          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii0gHFsoG-h6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv1d, MaxPool1d, Module, Softmax, BatchNorm1d, Dropout, Tanh\n",
        "from torch.optim import Adam, SGD\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm, trange\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "import math\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czCrUYFe1eOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8122f77b-d07e-41b5-e184-7278da2a238c"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"Running on the CPU\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3UTgmoWXmOe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "447f80ef-5cf5-44b4-ac11-8e8374657a90"
      },
      "source": [
        "DATASET = 'davis'\n",
        "\n",
        "def pre_process_SMILE(mode):\n",
        "\n",
        "  final_SMILES = []\n",
        "  char_num = {}\n",
        "  SMILES_length = 0\n",
        "  if mode == 'kiba':\n",
        "    SMILES_length = 100\n",
        "  if mode == 'davis':\n",
        "    SMILES_length = 95\n",
        "\n",
        "  with open(\"/content/data/\"+DATASET+\"/ligands_iso.txt\") as fp:\n",
        "    line = fp.readline()\n",
        "    SMILES = line.split(\",\")\n",
        "    final_SMILES = [j.replace('\"',\"\") for j in [k.replace(' ','') for k in [i.split(\":\")[1] for i in SMILES]]]\n",
        "    final_SMILES[-1] = final_SMILES[-1].replace(\"}\",\"\")\n",
        "  unique_set = list(set([j for i in final_SMILES for j in i]))\n",
        "  for n, i in enumerate(unique_set):\n",
        "    char_num[i] = n + 1\n",
        "  pre_procesed_SMILES = []\n",
        "  for i in final_SMILES:\n",
        "    x = []\n",
        "    for j in i:\n",
        "      x.append(char_num[j])\n",
        "    if len(x) > SMILES_length:\n",
        "      x = x[:SMILES_length]\n",
        "    elif len(x) < SMILES_length:\n",
        "      for j in range(SMILES_length - len(x)):\n",
        "        x.append(0)\n",
        "\n",
        "    pre_procesed_SMILES.append(x)\n",
        "  return pre_procesed_SMILES\n",
        "\n",
        "pre_procesed_SMILES = pre_process_SMILE(DATASET)\n",
        "print(len(pre_procesed_SMILES))\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs579JccplVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0575e845-e900-492f-a75b-7f484c5d9431"
      },
      "source": [
        "def pre_process_proteins(mode):\n",
        "  char_num = {}\n",
        "  proteins_length = 0\n",
        "  word_length = 3\n",
        "  if mode == 'kiba':\n",
        "    proteins_length = 450\n",
        "  if mode == 'davis':\n",
        "    proteins_length = 450\n",
        "\n",
        "  with open(\"/content/data/\"+DATASET+\"/proteins.txt\") as fp:\n",
        "    line = fp.readline()\n",
        "    proteins = line.split(\",\")\n",
        "    final_proteins = [j.replace('\"',\"\") for j in [k.replace(' ','') for k in [i.split(\":\")[1] for i in proteins]]]\n",
        "    final_proteins[-1] = final_proteins[-1].replace(\"}\",\"\")\n",
        "    print(\"number of proteins :\", len(final_proteins))\n",
        "\n",
        "  unique_word = {}\n",
        "  for protein in final_proteins:\n",
        "    for n in range(0,(len(protein)-word_length+1),word_length):\n",
        "      if protein[n:n+word_length] not in unique_word:\n",
        "        unique_word[protein[n:n+word_length]] = 1\n",
        "      else:\n",
        "        unique_word[protein[n:n+word_length]]+=1\n",
        "\n",
        "  unique_word = {k: v for k, v in sorted(unique_word.items(), key=lambda item: item[1])}\n",
        "  word_num = {}\n",
        "  for n, i in enumerate(list(unique_word)):\n",
        "    word_num[i] = n + 1\n",
        "  \n",
        "  pre_procesed_proteins = []\n",
        "  length = []\n",
        "  for protein in final_proteins:\n",
        "    x = []\n",
        "    for n in range(0,(len(protein)-word_length+1),word_length):\n",
        "      x.append(word_num[protein[n:n+word_length]])\n",
        "    length.append(len(x))\n",
        "    if len(x) > proteins_length:\n",
        "      x = x[:proteins_length]\n",
        "    elif len(x) < proteins_length:\n",
        "      for j in range(proteins_length - len(x)):\n",
        "        x.append(0)\n",
        "    pre_procesed_proteins.append(x)\n",
        "  return pre_procesed_proteins, length\n",
        "\n",
        "\n",
        "def check_good_length(length):\n",
        "  plt.hist(length, len(length))\n",
        "  plt.show()\n",
        "  selected = []\n",
        "  for i in length:\n",
        "    if i <= 450:\n",
        "      selected.append(i)\n",
        "  if len(selected)/len(length) > 0.85:\n",
        "    print(len(selected)/len(length),\"good\")\n",
        "\n",
        "pre_procesed_proteins, length = pre_process_proteins(DATASET)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of proteins : 442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B5czMzFYj9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class data(Dataset):\n",
        "  def __init__(self, data, y):\n",
        "    self.y = pd.DataFrame(y)\n",
        "    self.data = pd.DataFrame(data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \n",
        "    data = np.array(self.data.loc[index])\n",
        "    label = np.array(self.y.loc[index])\n",
        "    sample = (data ,label)\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    self.len = self.data.shape\n",
        "    return len(self.data)\n",
        "import random\n",
        "\n",
        "def over_under_sampling(major_part_indices, under_rate, minor_class_index, over_rate, middle_class_index, middle_rate):\n",
        "  \n",
        "  index_value = random.sample(major_part_indices,int(len(major_part_indices)/under_rate ))\n",
        "  x = []\n",
        "  for i in range(over_rate):\n",
        "    x+=minor_class_index\n",
        "  x1 = []\n",
        "  for i in range(middle_rate):\n",
        "    x1+=middle_class_index\n",
        "  \n",
        "  return index_value+x+x1\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "label = pickle.load(open(\"/content/data/\"+DATASET+\"/Y\", \"rb\"), encoding='latin1')\n",
        "label_f_no_log = label[~np.isnan(label)]\n",
        "if DATASET == \"davis\":\n",
        "  label_f = -(np.log10(label_f_no_log/(math.pow(10,9))+0.00000001))\n",
        "if DATASET == 'kiba':\n",
        "  label_f = label_f_no_log\n",
        "label_row_inds, label_col_inds = np.where(np.isnan(label)==False)\n",
        "test_fold = json.load(open(\"/content/data/\"+DATASET+\"/folds/test_fold_setting1.txt\"))\n",
        "train_fold = json.load(open(\"/content/data/\"+DATASET+\"/folds/train_fold_setting1.txt\"))\n",
        "\n",
        "validation_fold = train_fold[4]\n",
        "train_fold = [j for i in train_fold[:4] for j in i ]\n",
        "major_class_index = []  \n",
        "minor_class_index = []\n",
        "middle_class_index = []  \n",
        "train_y = []\n",
        "if DATASET == 'davis':\n",
        "  for i in train_fold:\n",
        "    if 4.9 <= label_f[i] <= 5.1:\n",
        "      major_class_index.append(i)  \n",
        "    else:\n",
        "      if  5.1 < label_f[i] < 7:\n",
        "        middle_class_index.append(i)\n",
        "      else:\n",
        "        minor_class_index.append(i)\n",
        "  new_train_fold = over_under_sampling(major_class_index,3,minor_class_index, 6, middle_class_index, 1)\n",
        "if DATASET == 'kiba':\n",
        "  new_train_fold = train_fold\n",
        "\n",
        "\n",
        "train_drug_indices = label_row_inds[new_train_fold]\n",
        "train_protein_indices = label_col_inds[new_train_fold]\n",
        "test_drug_indices = label_row_inds[test_fold]\n",
        "test_protein_indices = label_col_inds[test_fold]\n",
        "validation_drug_indices = label_row_inds[validation_fold]\n",
        "validation_protein_indices = label_col_inds[validation_fold]\n",
        "\n",
        "smile_train_data = []\n",
        "for i in train_drug_indices:\n",
        "  smile_train_data.append(pre_procesed_SMILES[i])\n",
        "\n",
        "protein_train_data = []\n",
        "for i in train_protein_indices:\n",
        "  protein_train_data.append(pre_procesed_proteins[i])\n",
        "\n",
        "smile_test_data = []\n",
        "for i in test_drug_indices:\n",
        "  smile_test_data.append(pre_procesed_SMILES[i])\n",
        "\n",
        "protein_test_data = []\n",
        "for i in test_protein_indices:\n",
        "  protein_test_data.append(pre_procesed_proteins[i])\n",
        "\n",
        "smile_validation_data = []\n",
        "for i in validation_drug_indices:\n",
        "  smile_validation_data.append(pre_procesed_SMILES[i])\n",
        "\n",
        "protein_validation_data = []\n",
        "for i in validation_protein_indices:\n",
        "  protein_validation_data.append(pre_procesed_proteins[i])\n",
        "  \n",
        "train_y = []\n",
        "for i in new_train_fold:\n",
        "  train_y.append(label_f[i])\n",
        "\n",
        "test_y = []\n",
        "for i in test_fold:\n",
        "  test_y.append(label_f[i])\n",
        "\n",
        "validation_y = []\n",
        "for i in validation_fold:\n",
        "  validation_y.append(label_f[i])\n",
        "  \n",
        "train_set_p = data(protein_train_data, train_y)\n",
        "train_loader_p = DataLoader(dataset=train_set_p,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True)\n",
        "\n",
        "train_set_s = data(smile_train_data, train_y)\n",
        "train_loader_s = DataLoader(dataset=train_set_s,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True)\n",
        "\n",
        "\n",
        "test_set_p = data(protein_test_data, test_y)\n",
        "test_loader_p = DataLoader(dataset=test_set_p,\n",
        "                        batch_size=batch_size,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)\n",
        "\n",
        "test_set_s = data(smile_test_data, test_y)\n",
        "test_loader_s = DataLoader(dataset=test_set_s,\n",
        "                        batch_size=batch_size,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)  \n",
        "\n",
        "\n",
        "validation_set_p = data(protein_validation_data, validation_y)\n",
        "validation_loader_p = DataLoader(dataset=validation_set_p,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)\n",
        "\n",
        "validation_set_s = data(smile_validation_data, validation_y)\n",
        "validation_loader_s = DataLoader(dataset=validation_set_s,\n",
        "                        batch_size=batch_size,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)  \n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gderbgQeWIZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(CNN, self).__init__()\n",
        "\n",
        "      self.cnn_layers = Sequential(\n",
        "          Conv1d(1, 32, kernel_size=4, stride=2, padding=0),\n",
        "          BatchNorm1d(32),\n",
        "          ReLU(inplace=True),\n",
        "          Conv1d(32, 64, kernel_size=8, stride=2, padding=0),\n",
        "          ReLU(inplace=True),\n",
        "          MaxPool1d(3, stride=2), \n",
        "              \n",
        "      )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.float()\n",
        "    x = self.cnn_layers(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    return x\n",
        "\n",
        "class CNN2(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(CNN2, self).__init__()\n",
        "\n",
        "      self.cnn_layers = Sequential(\n",
        "          Conv1d(1, 32, kernel_size=4, stride=2, padding=0),\n",
        "          BatchNorm1d(32),\n",
        "          ReLU(inplace=True),\n",
        "          Conv1d(32, 64, kernel_size=8, stride=2, padding=0),\n",
        "          BatchNorm1d(64),\n",
        "          ReLU(inplace=True),\n",
        "          Conv1d(64, 96, kernel_size=12, stride=2, padding=0),\n",
        "          ReLU(inplace=True),\n",
        "          MaxPool1d(4, stride=3), \n",
        "              \n",
        "      )\n",
        "      self.relu = nn.ReLU()\n",
        "      \n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x.float()\n",
        "    x = self.cnn_layers(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.input_dim = 1\n",
        "        self.hidden_dim = 256\n",
        "        self.target_dim = 1\n",
        "        self.batch_size = 256\n",
        "        self.p_len = 450\n",
        "        \n",
        "        if DATASET == \"davis\":\n",
        "          self.input_n = 576\n",
        "          self.s_len = 95\n",
        "        if DATASET == \"kiba\":\n",
        "          self.input_n = 640\n",
        "          self.s_len = 100\n",
        "          \n",
        "        self.cnn = CNN()\n",
        "        self.cnn2 = CNN2()\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, batch_first = True, num_layers = 1) \n",
        "        #384 448    \n",
        "        self.linear1 = nn.Linear(self.input_n + self.hidden_dim,1024)\n",
        "        self.linear2 = nn.Linear(1024, 512)\n",
        "        self.linear3 = nn.Linear(512, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = Dropout(0.1)\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "       \n",
        "        h0 = torch.zeros(1, self.batch_size, self.hidden_dim).requires_grad_()\n",
        "        h0 = h0.to(device)\n",
        "        c0 = torch.zeros(1, self.batch_size, self.hidden_dim).requires_grad_()\n",
        "        c0 = c0.to(device)\n",
        "\n",
        "        x1 = x1.float()\n",
        "        x2 = x2.float()\n",
        "\n",
        "        c_in = x2.view(self.batch_size, 1, self.s_len)\n",
        "        SMILEs = self.cnn(c_in)\n",
        "\n",
        "        c_in_2 = x1.view(self.batch_size, 1, self.p_len)\n",
        "        pre_proteins = self.cnn2(c_in_2)    \n",
        "        lstm_in = pre_proteins.view(len(x1), 1536, -1)\n",
        "        proteins, (hn, cn) = self.lstm(lstm_in, (h0.detach(), c0.detach()))\n",
        "       \n",
        "        proteins = proteins[:,-1,:]\n",
        "        concatination = torch.cat((SMILEs, proteins), 1)\n",
        "        out1 = self.linear1(concatination)\n",
        "        relu_out1 = self.relu(out1)\n",
        "        relu_out1 = self.dropout(relu_out1)\n",
        "        out2 = self.linear2(relu_out1)\n",
        "        relu_out2 = self.relu(out2)\n",
        "        relu_out2 = self.dropout(relu_out2)\n",
        "        y_pred = self.linear3(relu_out2)\n",
        "\n",
        "        \n",
        "        return y_pred\n",
        "\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvWiegkZODIL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "f9be760a-d0e4-4c46-f28d-239b9f126f07"
      },
      "source": [
        "\n",
        "def weighted_mse_loss(input, target, weight):\n",
        "    return torch.mean(torch.matmul(weight, (input - target)**2))\n",
        "\n",
        "final_model = MyModel()\n",
        "final_model = final_model.to(device)\n",
        "print(final_model)\n",
        "if DATASET == 'davis':\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion.to(device)\n",
        "if DATASET == 'kiba':\n",
        "  criterion = weighted_mse_loss\n",
        "\n",
        "optimizer = torch.optim.AdamW(final_model.parameters(), lr = 0.0001)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyModel(\n",
            "  (cnn): CNN(\n",
            "    (cnn_layers): Sequential(\n",
            "      (0): Conv1d(1, 32, kernel_size=(4,), stride=(2,))\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv1d(32, 64, kernel_size=(8,), stride=(2,))\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (cnn2): CNN2(\n",
            "    (cnn_layers): Sequential(\n",
            "      (0): Conv1d(1, 32, kernel_size=(4,), stride=(2,))\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv1d(32, 64, kernel_size=(8,), stride=(2,))\n",
            "      (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Conv1d(64, 96, kernel_size=(12,), stride=(2,))\n",
            "      (7): ReLU(inplace=True)\n",
            "      (8): MaxPool1d(kernel_size=4, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (lstm): LSTM(1, 256, batch_first=True)\n",
            "  (linear1): Linear(in_features=832, out_features=1024, bias=True)\n",
            "  (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (linear3): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84EJQ-MQOYMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "705d7759-533a-4783-d63d-0cba6ba2dbec"
      },
      "source": [
        "epoch_num = 15\n",
        "\n",
        "train_log = []\n",
        "val_log = []\n",
        "x = 0\n",
        "\n",
        "for epoch in range(1, epoch_num+1):\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  final_model.train()\n",
        "  for batch_idx1, ((datap, target), (datas, target)) in enumerate(zip(train_loader_p, train_loader_s)):  \n",
        "    datap = datap.to(device)\n",
        "    datas = datas.to(device)\n",
        "    target = target.float()\n",
        "    target = target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = final_model(datap,datas)\n",
        "    w = []\n",
        "    for i in target:\n",
        "      if 11 <= i <= 12.5:\n",
        "        w.append(0.8)\n",
        "      else:\n",
        "        w.append(1.2) \n",
        "    w = Variable(torch.FloatTensor(w), requires_grad = True).to(device)\n",
        "    if DATASET == 'davis':\n",
        "      loss = criterion(output, target)\n",
        "    if DATASET == 'kiba':\n",
        "      loss = criterion(output, target,w)\n",
        "    loss.backward()                \n",
        "    optimizer.step() \n",
        "    train_loss.append(loss.item()) \n",
        "  print('>> Epoch: ',epoch,\">> Train loss: \",np.mean(train_loss))\n",
        "  train_log.append(np.mean(train_loss))\n",
        " \n",
        "for batch_idx1, ((datap, target), (datas, target)) in enumerate(zip(validation_loader_p, validation_loader_s)):  \n",
        "  datap = datap.to(device)\n",
        "  datas = datas.to(device)\n",
        "  target = target.float()\n",
        "  target = target.to(device)\n",
        "  output = final_model(datap,datas)\n",
        "  if DATASET == 'davis':\n",
        "      loss = criterion(output, target)\n",
        "  if DATASET == 'kiba':\n",
        "    loss = criterion(output, target,w)\n",
        "  val_loss.append(loss.item()) \n",
        "print(\">> Validation loss: \",np.mean(val_loss))\n",
        "val_log.append(np.mean(val_loss))\n"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Epoch:  1 >> Train loss:  9.536286131976402\n",
            ">> Epoch:  2 >> Train loss:  1.3117984516979897\n",
            ">> Epoch:  3 >> Train loss:  1.2100816671162435\n",
            ">> Epoch:  4 >> Train loss:  1.1256956025345686\n",
            ">> Epoch:  5 >> Train loss:  1.0643657045821622\n",
            ">> Epoch:  6 >> Train loss:  1.028726242176474\n",
            ">> Epoch:  7 >> Train loss:  1.0077253163677373\n",
            ">> Epoch:  8 >> Train loss:  1.0016438013886753\n",
            ">> Epoch:  9 >> Train loss:  0.9978334119875137\n",
            ">> Epoch:  10 >> Train loss:  0.9940990152424329\n",
            ">> Epoch:  11 >> Train loss:  0.9953964589393303\n",
            ">> Epoch:  12 >> Train loss:  0.9899729180009398\n",
            ">> Epoch:  13 >> Train loss:  0.9847485153642419\n",
            ">> Epoch:  14 >> Train loss:  0.9924592914646619\n",
            ">> Epoch:  15 >> Train loss:  0.9867014754308413\n",
            ">> Validation loss:  1.1038782439733807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxH3wUihkYb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "eafc850a-081b-486b-9198-32a774f6f3c7"
      },
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    final_model.eval()\n",
        "    for batch_idx1, ((datap, labels), (datas, labels)) in enumerate(zip(test_loader_p, test_loader_s)):  \n",
        "        datap = datap.to(device)\n",
        "        datas = datas.to(device)\n",
        "        target = target.float()\n",
        "        target = target.to(device)      \n",
        "        outputs = final_model(datap, datas) \n",
        "        y_true += labels.tolist()\n",
        "        y_pred += outputs.tolist()\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "print(len(y_true))\n",
        "print(len(y_pred))\n",
        "mse = np.square(np.subtract(y_true,y_pred)).mean() \n",
        "print('MSE: ', mse)\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4864\n",
            "4864\n",
            "MSE:  1.0954824523253428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0amGsbUmxP8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46cc154d-99e3-4001-f758-a6540076cc6c"
      },
      "source": [
        "def get_cindex(Y, P):\n",
        "    summ = 0\n",
        "    pair = 0\n",
        "    \n",
        "    for i in range(1, len(Y)):\n",
        "        for j in range(0, i):\n",
        "            if i is not j:\n",
        "                if(Y[i] > Y[j]):\n",
        "                    pair +=1\n",
        "                    summ +=  1* (P[i] > P[j]) + 0.5 * (P[i] == P[j])\n",
        "        \n",
        "            \n",
        "    if pair is not 0:\n",
        "        return summ/pair\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "CI = get_cindex(y_true, y_pred)\n",
        "print(\"CI: \",CI)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CI:  [0.72238992]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoUsliKthcsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "351c6969-86aa-43e4-dcf9-cc496520dde1"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "  TP, TN, FP, FN = 0, 0, 0, 0\n",
        "  for i,j in zip(y_true, y_pred):\n",
        "    if i == j == 1:\n",
        "      TP+=1\n",
        "    if i == j == 0:\n",
        "      TN+=1\n",
        "    if i == 0 and j == 1:\n",
        "      FP+=1\n",
        "    if i == 1 and j == 0:\n",
        "      FN+=1\n",
        "  return TP, TN, FP, FN\n",
        "\n",
        "if DATASET == 'davis':\n",
        "  y_true_classification = [1 if i>=7 else 0 for i in y_true]\n",
        "  y_pred_classification = [1 if i>=7 else 0 for i in y_pred]\n",
        "\n",
        "if DATASET == 'kiba':\n",
        "  y_true_classification = [1 if i>=12.1 else 0 for i in y_true]\n",
        "  y_pred_classification = [1 if i>=12.1 else 0 for i in y_pred]\n",
        "\n",
        "print(len(y_true_classification))\n",
        "print(len(y_pred_classification))\n",
        "\n",
        "f1_score = f1_score(y_true_classification, y_pred_classification)\n",
        "a = precision_recall_fscore_support(y_true_classification, y_pred_classification, average='macro')\n",
        "precision = a[0]\n",
        "recall = a[1]\n",
        "TP, TN, FP, FN = confusion_matrix(y_true_classification, y_pred_classification)\n",
        "print(TP, TN, FP, FN)\n",
        "sensitivity = TP/(TP+ FN)\n",
        "specifity = TN/( TN + FP )\n",
        "acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(sensitivity, specifity, acc, f1_score)\n",
        "print(y_pred_classification)\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4864\n",
            "4864\n",
            "153 4066 412 233\n",
            "0.3963730569948187 0.9079946404644931 0.8673930921052632 0.3217665615141956\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKS1PZHenZFi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "b1127463-e49b-4118-894d-3a8c7705cce8"
      },
      "source": [
        "# x = [10 for i in train_y if i>= 11 and i<= 12]\n",
        "plt.hist(train_y, 20)\n",
        "plt.xlabel('binding values') \n",
        "plt.ylabel('numbers')\n",
        "plt.title('distribution of binding values in davis dataset before resampling') \n",
        "plt.savefig(\"f.jpg\")\n",
        "plt.show() \n",
        "plt.show()\n",
        "label_f_x = label_f / np.linalg.norm(label_f)\n",
        "\n",
        "plt.hist(test_y, 10)\n",
        "plt.show()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8SbfxAnj1Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(final_model.state_dict(), \"/content/myModel_final_d\")"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jvoWkNcgtz5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "319b42b7-3c5f-49ed-bd28-10c0f745a273"
      },
      "source": [
        "\n",
        "final_model = MyModel()\n",
        "final_model.load_state_dict(torch.load(\"/content/myModel_final_d\"))\n",
        "# final_model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "device = torch.device(\"cuda\")\n",
        "final_model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    final_model.eval()\n",
        "    for batch_idx1, ((datap, labels), (datas, labels)) in enumerate(zip(test_loader_p, test_loader_s)):  \n",
        "        datap = datap.to(device)\n",
        "        datas = datas.to(device)\n",
        "        target = target.float()\n",
        "        target = target.to(device)      \n",
        "        outputs = final_model(datap, datas) \n",
        "        y_true += labels.tolist()\n",
        "        y_pred += outputs.tolist()\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "print(len(y_true))\n",
        "print(len(y_pred))\n",
        "mse = np.square(np.subtract(y_true,y_pred)).mean() \n",
        "print('MSE: ', mse)\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4864\n",
            "4864\n",
            "MSE:  1.2707435469640374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI9TGsI7HpbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}